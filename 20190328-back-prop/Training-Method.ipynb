{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Method\n",
    "\n",
    "\n",
    "- 5장부터 9장까지는 Training method에 대한 설명\n",
    "\n",
    "\n",
    "- 여러 방법에 대한 짧은 소개가 대부분(자세한 설명x)\n",
    "\n",
    "\n",
    "- 3가지를 주로 설명할 것\n",
    "\n",
    "\n",
    "- 수식은 거의 다 다른 논문에서 인용\n",
    "\n",
    "\n",
    "<img src = https://i.imgur.com/JRW6g8d.png width = 400>\n",
    "\n",
    "```\n",
    "\n",
    "-5. Gradient descent 설명\n",
    "\n",
    "\n",
    "-6. Second-order method 설명\n",
    "\n",
    "\n",
    "-7. 멀티레이어에서 쓰이는 헤시안 행렬 설명\n",
    "\n",
    "\n",
    "-8. 다중레이어 헤시안으로 분석\n",
    "\n",
    "\n",
    "-9. 다중레이어 신경망에 Second-order method 적용해보기\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## Index\n",
    "\n",
    "\n",
    "* SGD  (5-2,5-3)\n",
    "\n",
    "\n",
    "* Gauss-Newton (6-4)\n",
    "\n",
    "\n",
    "* Levenberg Marquardt (7-2, 9-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Convergence of Gradient Descent\n",
    "\n",
    "### 5-1 A Little Theory\n",
    "\n",
    "- 1차원 gradient descent\n",
    "\n",
    "\n",
    "- 최적의 learning rate 찾기\n",
    "\n",
    "\n",
    "- learing rate 범위를 어떻게 봐야 하는가\n",
    "\n",
    "\n",
    "- 최적의 learning rate보다 작게하면 오래걸리고 크게 하면 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://i.imgur.com/6mukIph.png width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://i.imgur.com/sgPXutW.png width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이후에 나오는 수식들은 최적화 가중치를 찾는 과정들\n",
    "\n",
    "\n",
    "- 처음에는 1차원 수식 설명\n",
    "\n",
    "\n",
    "- 1차원에서 optimal learning rate 찾기\n",
    "\n",
    "\n",
    "- 2차원에서 가중치 찾기\n",
    "\n",
    "\n",
    "- 2차원에서 optimal learning rate 찾기\n",
    "\n",
    "\n",
    "- multiple dimensions에서 optimal learning rate 찾기\n",
    "\n",
    "\n",
    "- error function으로 LMS 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오차 제곱합이 최소화 되도록 모델 파라미터 p를 정하는 방법을 최소자승법(least square method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1차원일 때 loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://i.imgur.com/JRW6g8d.png width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최적의 Weight\n",
    "\n",
    "<img src = https://i.imgur.com/Qfre6T5.png width = 400>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 Examples\n",
    "\n",
    "#### Linear Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://i.imgur.com/FW4af54.png width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가\n",
    "### SGD (Stochastic  Gradient Descent)\n",
    "\n",
    "- 확률 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch mode 대신 SGD 사용 \n",
    "\n",
    "- (구체적인 설명이 없어서 아쉬움)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://i.imgur.com/jkAwfFC.png width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://i.imgur.com/SzvaSyz.png width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3 Input Transformations and Error Surface Transformations Revisited\n",
    "\n",
    "\n",
    "1. 입력 변수에서 평균 빼기\n",
    "\n",
    "2. 입력 변수의 분산 정규화\n",
    "\n",
    "3. 입력 변수의 상관관계 확인\n",
    "\n",
    "4. 각각의 가중치에 learning rate를 따로 사용해라"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고\n",
    "### Frist-order optimization\n",
    "\n",
    "- Gradient descent 알고리즘의 변형 알고리즘들\n",
    "- Momentum, NAG, AdaGrad, AdaDelta, RMSProp, Adam 등\n",
    "\n",
    "### Second order optimization\n",
    "\n",
    "- Gradient descent 알고리즘의 변형 알고리즘들\n",
    "- 비싼 작업이라 잘 사용 안함\n",
    "- 왜 비쌀까? Hessian matrix 라는 2차 편미분 행렬을 계산한 후 역행렬을 구하기 때문\n",
    "\n",
    "#### first와 second를 나누는 기준은 미분을 한 번 했냐 두 번했냐의 차이\n",
    "\n",
    "- 2차 편미분이 포함된 식은 second\n",
    "- 1차 편마분만 포함된 식은 first\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Classical second order optimization methods\n",
    "\n",
    "- second-order optimization methods가 왜 imparctical 한지 증명하지는 않을것\n",
    "\n",
    "### 6-1 Newton Algorithm\n",
    "\n",
    "- 뉴턴 알고리즘은 21번에서 23번을 빼준것\n",
    "\n",
    "- error function이 2차원일때 one step으로 수렴한다\n",
    "\n",
    "#### 참고 사이트 \n",
    "* [뉴턴법/뉴턴-랩슨법의 이해와 활용(Newton's method)](https://darkpgmr.tistory.com/58)\n",
    "\n",
    "뉴턴 알고리즘과 gradient descent 비교\n",
    "<img src = https://i.imgur.com/TXmLscq.png width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2 Conjugate Gradient\n",
    "\n",
    "- 켤레기울기법 또는 공역기울기법\n",
    "\n",
    "\n",
    "- 수학에서 대칭인 양의 준정부호행렬(陽-準定符號行列, 영어: positive-semidefinite matrix)을 갖는 선형계의 해를 구하는 수치 알고리즘\n",
    "\n",
    "\n",
    "- 헤시안 행렬을 사용하지않음 (왜?)\n",
    "\n",
    "\n",
    "- O(N) method 사용\n",
    "\n",
    "\n",
    "- bactch mode 사용\n",
    "\n",
    "\n",
    "<img src = https://i.imgur.com/FWCFVjT.png width = 400>\n",
    "\n",
    "\n",
    "first gradient descent에서 직각을 찾음\n",
    "\n",
    "\n",
    "최소화하는 gradient descent direction을 찾음\n",
    "\n",
    "벡터에서 다시 직각이 되는 gradient를 찾음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://i.imgur.com/Btrof8P.png width = 500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://i.imgur.com/vXr3MMy.png width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://i.imgur.com/Tsa365s.png width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3 Quasi-Newton (BFGS)\n",
    "\n",
    "- 헤시안 역행렬의 추정을 이용해 계산하는 방법\n",
    "\n",
    "- 오직 batch learning만 사용 (mini-batch 사용안함)\n",
    "\n",
    "- O(N^2) 알고리즘 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M = positive definite Matrix\n",
    "\n",
    "\n",
    "M = I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =https://i.imgur.com/eiMpFDS.png width = 200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-4 Gauss-Newton and Levenberg Marquardt\n",
    "\n",
    "- Levenberg–Marquardt 방법은 가우스-뉴턴법(Gauss–Newton method)과 Gradient descent 방법이 결합된 형태로 볼 수 있다.\n",
    "- Levenberg–Marquardt를 사용하거나 이해하기 위해서는 위 두가지 방법을 알아야한다,\n",
    "\n",
    "\n",
    "- Gauss-Newton and Levenberg Marquardt는 Jacobian(자코비언) 근사를 사용한다.\n",
    "- 또한 주로 batch learning을 사용\n",
    "- O(N^2) 알고리즘 사용\n",
    "- MSE를 loss function으로 사용\n",
    "\n",
    "<img src = https://i.imgur.com/9XugirZ.png width = 400>\n",
    "\n",
    "#### 참고 사이트\n",
    "\n",
    "* [Levenberg-Marquardt 방법](http://blog.naver.com/PostView.nhn?blogId=tlaja&logNo=220735045887&categoryNo=42&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView)\n",
    "\n",
    "\n",
    "* [자코비안(Jacobian)이란 무엇인가](http://t-robotics.blogspot.com/2013/12/jacobian.html#.XJtLB5gzY2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Tricks to compute the Hessian information in multilayer networks\n",
    "\n",
    "\n",
    "- 멀리레이터 신경망에서 계산하는 트릭\n",
    "\n",
    "\n",
    "#### 7-1 Finite Difference\n",
    "\n",
    "- 헤시안 정의\n",
    "\n",
    "<img src = https://i.imgur.com/dyhwlXP.png width = 300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-2 Square Jacobian approximation for the Gauss-Newton and Levenberg-Marquardt algorithms\n",
    "\n",
    "가우스 뉴턴과 리번버그 말콸디에프 알고리즘\n",
    "\n",
    "\n",
    "loss function\n",
    "\n",
    "<img src = https://i.imgur.com/mAwuyQQ.png width = 400>\n",
    "\n",
    "Gradient\n",
    "\n",
    "<img src = https://i.imgur.com/fhx9qgR.png width = 400>\n",
    "\n",
    "\n",
    "<img src = https://i.imgur.com/BKoX4Yx.png width = 400>\n",
    "\n",
    "\n",
    "#### 7-3 Backpropagating second derivatives\n",
    "\n",
    "- forward to compute this matrix\n",
    "\n",
    "<img src = https://i.imgur.com/d1zERhq.png width = 400>\n",
    "\n",
    "\n",
    "\n",
    "#### 7-4 Backpropagating the diagonal Hessian in neural nets\n",
    "\n",
    "- 가우스-뉴턴 근사를 이용해 대각 헤시안 행렬을 구해보자\n",
    "\n",
    "<img src = https://i.imgur.com/J2wQArC.png width = 400>\n",
    "\n",
    "\n",
    "#### 7-5 Computing the product of the Hessian and a vector\n",
    "- 헤시안 행렬 계산 방법\n",
    "\n",
    "<img src = https://i.imgur.com/vKH1BOX.png width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 Analysis of the Hessian in multi-layer networks\n",
    "\n",
    "- 다층 신경망에서 헤시안행렬 분석\n",
    "\n",
    "트레인 과정에서 large eigenvalues에서 발생하는 문제들에 대해 논증\n",
    "\n",
    "데이터 : OCR\n",
    "\n",
    "<img src = https://i.imgur.com/UlWWy62.png width = 500>\n",
    "\n",
    "눈에 잘 안들어온다\n",
    "\n",
    "보기 쉽게 log를 취해줌\n",
    "<img src = https://i.imgur.com/mb7s30p.png width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 Applying Second Order Methods to Multilayer Networks\n",
    "\n",
    "- 다층 신경망에 second-order method 적용해보기\n",
    "\n",
    "<img src = https://i.imgur.com/6zPgTGL.png width = 400>\n",
    "\n",
    "\n",
    "- Full batch말고 mini-batch 쓰자\n",
    "\n",
    "\n",
    "#### 9-1 A stochastic diagonal Levenberg Marquardf method\n",
    "\n",
    "\n",
    "#### 9-2 Computing the principal Eigenvalue/vector of the Hessian\n",
    "증명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
